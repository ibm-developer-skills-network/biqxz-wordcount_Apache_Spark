{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"wordcount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Unofficial Transl...|\n",
      "|                    |\n",
      "|     Monetary Policy|\n",
      "|         for 2023/24|\n",
      "|                    |\n",
      "|   Nepal Rastra Bank|\n",
      "|      Central Office|\n",
      "|Baluwatar, Kathmandu|\n",
      "|           July 2023|\n",
      "|                    |\n",
      "| \\f\\fMonetary Policy|\n",
      "|         for 2023/24|\n",
      "|                    |\n",
      "|        Delivered by|\n",
      "|Governor Mr. Maha...|\n",
      "|                  On|\n",
      "|        23 July 2023|\n",
      "|                    |\n",
      "|   Nepal Rastra Bank|\n",
      "|      Central Office|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(r\"D:\\my_project\\word_count_with_Apache_Spark\\Monetary-policy-in-English-2023_24-Full-text.txt\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|word    |count|\n",
      "+--------+-----+\n",
      "|the     |983  |\n",
      "|to      |456  |\n",
      "|of      |442  |\n",
      "|and     |365  |\n",
      "|in      |270  |\n",
      "|for     |167  |\n",
      "|be      |162  |\n",
      "|a       |140  |\n",
      "|percent |135  |\n",
      "|policy  |125  |\n",
      "|on      |112  |\n",
      "|monetary|110  |\n",
      "|rate    |105  |\n",
      "|has     |98   |\n",
      "|sector  |91   |\n",
      "|is      |90   |\n",
      "|as      |90   |\n",
      "|by      |90   |\n",
      "|foreign |80   |\n",
      "|bank    |78   |\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Split lines into words\n",
    "words = df.select(F.explode(F.split(df.value, \" \")).alias(\"word\"))\n",
    "\n",
    "# Clean the words (optional): Remove punctuation and convert to lowercase\n",
    "cleaned_words = words.withColumn(\"word\", F.lower(F.regexp_replace(\"word\", \"[^a-zA-Z]\", \"\")))\n",
    "\n",
    "# Filter out empty strings\n",
    "cleaned_words = cleaned_words.filter(cleaned_words.word != '')\n",
    "\n",
    "# Count the words\n",
    "word_counts = cleaned_words.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show the top 20 words by count\n",
    "word_counts.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
